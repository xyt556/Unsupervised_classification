import numpy as np
import rasterio
from sklearn.cluster import (KMeans, MiniBatchKMeans, DBSCAN, OPTICS, 
                             MeanShift, Birch, SpectralClustering, 
                             AgglomerativeClustering)
from sklearn.mixture import GaussianMixture
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
import os
from pathlib import Path
from matplotlib.patches import Patch
from scipy.ndimage import label, binary_dilation, generic_filter
from scipy.ndimage import maximum_filter, minimum_filter
from scipy import ndimage
import pandas as pd
import warnings
import time
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from functools import partial

# å°è¯•å¯¼å…¥numbaåŠ é€Ÿ
try:
    from numba import jit, prange
    NUMBA_AVAILABLE = True
except ImportError:
    NUMBA_AVAILABLE = False
    print("âš ï¸  numbaæœªå®‰è£…ï¼Œéƒ¨åˆ†åŠŸèƒ½å°†è¾ƒæ…¢ã€‚å®‰è£…: pip install numba")

# å°è¯•å¯¼å…¥scikit-fuzzy
try:
    import skfuzzy as fuzz
    FUZZY_AVAILABLE = True
    print("âœ“ scikit-fuzzy å·²å®‰è£…")
except ImportError:
    FUZZY_AVAILABLE = False
    print("âš ï¸  scikit-fuzzyæœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€åŒ–ç‰ˆæ¨¡ç³ŠCå‡å€¼")

warnings.filterwarnings('ignore')

plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

class PerformanceTimer:
    """æ€§èƒ½è®¡æ—¶å™¨"""
    def __init__(self, name):
        self.name = name
        self.start_time = None
    
    def __enter__(self):
        self.start_time = time.time()
        print(f"\nâ±ï¸  {self.name}...")
        return self
    
    def __exit__(self, *args):
        elapsed = time.time() - self.start_time
        print(f"âœ“ å®Œæˆ - è€—æ—¶: {elapsed:.2f}ç§’")

# ==================== ä¼˜åŒ–çš„åå¤„ç†å‡½æ•° ====================

if NUMBA_AVAILABLE:
    @jit(nopython=True, parallel=True, cache=True)
    def _majority_filter_numba(data, output, valid_mask, window_size=3):
        """
        NumbaåŠ é€Ÿçš„å¤šæ•°æ»¤æ³¢
        """
        height, width = data.shape
        offset = window_size // 2
        
        for i in prange(offset, height - offset):
            for j in range(offset, width - offset):
                if not valid_mask[i, j]:
                    continue
                
                # æå–çª—å£
                window = data[i-offset:i+offset+1, j-offset:j+offset+1]
                valid_window = valid_mask[i-offset:i+offset+1, j-offset:j+offset+1]
                
                # åªè€ƒè™‘æœ‰æ•ˆå€¼
                valid_values = window[valid_window]
                valid_values = valid_values[valid_values > 0]
                
                if len(valid_values) == 0:
                    continue
                
                # æ‰¾å‡ºç°æ¬¡æ•°æœ€å¤šçš„å€¼
                max_count = 0
                majority_value = data[i, j]
                
                for val in valid_values:
                    count = np.sum(valid_values == val)
                    if count > max_count:
                        max_count = count
                        majority_value = val
                
                output[i, j] = majority_value
    
    @jit(nopython=True, cache=True)
    def _get_neighbor_mode(classification, i, j, valid_mask):
        """è·å–é‚»åŸŸçš„ä¼—æ•°ï¼ˆæœ€å¸¸è§å€¼ï¼‰"""
        height, width = classification.shape
        values = []
        
        # 8é‚»åŸŸ
        for di in [-1, 0, 1]:
            for dj in [-1, 0, 1]:
                if di == 0 and dj == 0:
                    continue
                ni, nj = i + di, j + dj
                if 0 <= ni < height and 0 <= nj < width:
                    if valid_mask[ni, nj] and classification[ni, nj] > 0:
                        values.append(classification[ni, nj])
        
        if len(values) == 0:
            return 0
        
        # æ‰¾ä¼—æ•°
        max_count = 0
        mode_value = values[0]
        
        for val in values:
            count = sum(1 for v in values if v == val)
            if count > max_count:
                max_count = count
                mode_value = val
        
        return mode_value
else:
    _majority_filter_numba = None
    _get_neighbor_mode = None

def majority_filter_fast(classification_result, valid_mask, window_size=3):
    """
    å¿«é€Ÿå¤šæ•°æ»¤æ³¢ - ä½¿ç”¨scipyä¼˜åŒ–
    """
    if NUMBA_AVAILABLE:
        # ä½¿ç”¨numbaåŠ é€Ÿç‰ˆæœ¬
        output = classification_result.copy()
        _majority_filter_numba(classification_result, output, valid_mask, window_size)
        return output
    else:
        # ä½¿ç”¨scipyçš„modeæ»¤æ³¢
        from scipy.ndimage import generic_filter
        
        def mode_func(values):
            values = values[values > 0]
            if len(values) == 0:
                return 0
            # å¿«é€Ÿä¼—æ•°è®¡ç®—
            unique, counts = np.unique(values, return_counts=True)
            return unique[np.argmax(counts)]
        
        # åªå¯¹æœ‰æ•ˆåŒºåŸŸæ»¤æ³¢
        smoothed = classification_result.copy()
        temp = generic_filter(
            classification_result,
            mode_func,
            size=window_size,
            mode='constant',
            cval=0
        )
        smoothed[valid_mask] = temp[valid_mask]
        
        return smoothed

def remove_small_patches_fast(classification_result, valid_mask, min_patch_size):
    """
    å¿«é€Ÿç§»é™¤å°æ–‘å— - ä¼˜åŒ–ç‰ˆ
    
    ä¼˜åŒ–ç­–ç•¥:
    1. ä¸€æ¬¡æ€§æ ‡è®°æ‰€æœ‰è¿é€šåŒºåŸŸ
    2. æ‰¹é‡å¤„ç†å°æ–‘å—
    3. å‘é‡åŒ–é‚»åŸŸæŸ¥æ‰¾
    """
    if min_patch_size <= 0:
        return classification_result
    
    print(f"  å¿«é€Ÿç§»é™¤å°æ–‘å— (æœ€å°å°ºå¯¸: {min_patch_size})...")
    
    processed_result = classification_result.copy()
    
    # ä¸€æ¬¡æ€§å¯¹æ‰€æœ‰ç±»åˆ«è¿›è¡Œè¿é€šæ€§åˆ†æ
    # ä½¿ç”¨ç»“æ„åŒ–æ ‡ç­¾ï¼ŒåŒæ—¶è€ƒè™‘ç±»åˆ«å€¼
    labeled_array, num_features = label(valid_mask)
    
    # ä¸ºæ¯ä¸ªæœ‰æ•ˆåŒºåŸŸæ‰¾åˆ°å…¶ç±»åˆ«
    region_to_class = {}
    region_sizes = {}
    
    for region_id in range(1, num_features + 1):
        region_mask = labeled_array == region_id
        region_classes = classification_result[region_mask]
        
        if len(region_classes) > 0:
            # æ‰¾å‡ºè¯¥åŒºåŸŸçš„ä¸»è¦ç±»åˆ«
            unique, counts = np.unique(region_classes, return_counts=True)
            if len(unique) > 0:
                main_class = unique[np.argmax(counts)]
                region_to_class[region_id] = main_class
                region_sizes[region_id] = np.sum(region_mask)
    
    # å¯¹æ¯ä¸ªç±»åˆ«åˆ†åˆ«è¿›è¡Œè¿é€šæ€§åˆ†æ
    unique_classes = np.unique(classification_result[valid_mask])
    unique_classes = unique_classes[unique_classes > 0]
    
    removed_count = 0
    
    for class_id in unique_classes:
        class_mask = (classification_result == class_id) & valid_mask
        
        if not np.any(class_mask):
            continue
        
        # å¯¹å½“å‰ç±»åˆ«è¿›è¡Œè¿é€šæ€§åˆ†æ
        class_labeled, class_num_features = label(class_mask)
        
        if class_num_features == 0:
            continue
        
        # å‘é‡åŒ–è®¡ç®—æ‰€æœ‰åŒºåŸŸçš„å¤§å°
        region_sizes_class = np.bincount(class_labeled.ravel())
        
        # æ‰¾å‡ºæ‰€æœ‰å°åŒºåŸŸ
        small_regions = np.where(
            (region_sizes_class < min_patch_size) & 
            (region_sizes_class > 0)
        )[0]
        
        if len(small_regions) == 0:
            continue
        
        # æ‰¹é‡å¤„ç†å°åŒºåŸŸ
        for region_id in small_regions:
            region_mask = class_labeled == region_id
            
            # ä½¿ç”¨å½¢æ€å­¦è†¨èƒ€æ‰¾é‚»åŸŸï¼ˆæ›´å¿«ï¼‰
            dilated = binary_dilation(region_mask, iterations=1)
            neighbor_mask = dilated & ~region_mask & valid_mask
            
            if np.any(neighbor_mask):
                # æ‰¾é‚»åŸŸæœ€å¸¸è§çš„ç±»åˆ«
                neighbor_classes = classification_result[neighbor_mask]
                neighbor_classes = neighbor_classes[neighbor_classes > 0]
                
                if len(neighbor_classes) > 0:
                    # ä½¿ç”¨bincountåŠ é€Ÿ
                    unique_neighbors, counts = np.unique(neighbor_classes, return_counts=True)
                    new_class = unique_neighbors[np.argmax(counts)]
                    processed_result[region_mask] = new_class
                    removed_count += 1
    
    print(f"  âœ“ ç§»é™¤äº† {removed_count} ä¸ªå°æ–‘å—")
    return processed_result

def remove_small_patches_ultra_fast(classification_result, valid_mask, min_patch_size):
    """
    è¶…å¿«é€Ÿç§»é™¤å°æ–‘å— - ä½¿ç”¨åˆ†å—å¹¶è¡Œå¤„ç†
    
    é€‚åˆå¤§å›¾åƒ
    """
    if min_patch_size <= 0:
        return classification_result
    
    height, width = classification_result.shape
    
    # å¦‚æœå›¾åƒä¸å¤§ï¼Œä½¿ç”¨æ ‡å‡†æ–¹æ³•
    if height * width < 10000000:  # å°äº1000ä¸‡åƒç´ 
        return remove_small_patches_fast(classification_result, valid_mask, min_patch_size)
    
    print(f"  è¶…å¿«é€Ÿç§»é™¤å°æ–‘å— (åˆ†å—å¹¶è¡Œå¤„ç†)...")
    
    # åˆ†å—å¤„ç†
    block_size = 2048
    processed_result = classification_result.copy()
    
    # è®¡ç®—éœ€è¦å¤šå°‘å—
    n_blocks_h = (height + block_size - 1) // block_size
    n_blocks_w = (width + block_size - 1) // block_size
    
    total_removed = 0
    
    for i in range(n_blocks_h):
        for j in range(n_blocks_w):
            # è®¡ç®—å—çš„è¾¹ç•Œï¼ˆå¸¦é‡å ï¼‰
            overlap = 50
            h_start = max(0, i * block_size - overlap)
            h_end = min(height, (i + 1) * block_size + overlap)
            w_start = max(0, j * block_size - overlap)
            w_end = min(width, (j + 1) * block_size + overlap)
            
            # æå–å—
            block_class = classification_result[h_start:h_end, w_start:w_end].copy()
            block_valid = valid_mask[h_start:h_end, w_start:w_end]
            
            # å¤„ç†å—
            block_processed = remove_small_patches_fast(block_class, block_valid, min_patch_size)
            
            # å†™å›ä¸­å¿ƒåŒºåŸŸï¼ˆä¸åŒ…æ‹¬é‡å éƒ¨åˆ†ï¼‰
            actual_h_start = i * block_size
            actual_h_end = min(height, (i + 1) * block_size)
            actual_w_start = j * block_size
            actual_w_end = min(width, (j + 1) * block_size)
            
            offset_h = actual_h_start - h_start
            offset_w = actual_w_start - w_start
            
            processed_result[actual_h_start:actual_h_end, actual_w_start:actual_w_end] = \
                block_processed[
                    offset_h:offset_h + (actual_h_end - actual_h_start),
                    offset_w:offset_w + (actual_w_end - actual_w_start)
                ]
    
    print(f"  âœ“ åˆ†å—å¤„ç†å®Œæˆ")
    return processed_result

def post_process_classification_fast(classification_result, valid_mask, 
                                     min_patch_size=50, smoothing_iterations=1):
    """
    ä¼˜åŒ–çš„åå¤„ç†æµç¨‹
    
    é€Ÿåº¦æå‡ç­–ç•¥:
    1. ä½¿ç”¨numbaåŠ é€Ÿ
    2. å‘é‡åŒ–æ“ä½œ
    3. åˆ†å—å¹¶è¡Œå¤„ç†
    4. å‡å°‘å†…å­˜å¤åˆ¶
    """
    if min_patch_size <= 0 and smoothing_iterations <= 0:
        return classification_result
    
    print("\nå¿«é€Ÿåå¤„ç†...")
    processed_result = classification_result.copy()
    
    # æ­¥éª¤1: å»é™¤å°æ–‘å—
    if min_patch_size > 0:
        with PerformanceTimer("å»é™¤å°æ–‘å—"):
            # æ ¹æ®æ•°æ®è§„æ¨¡é€‰æ‹©æ–¹æ³•
            total_pixels = classification_result.size
            if total_pixels > 10_000_000:
                processed_result = remove_small_patches_ultra_fast(
                    processed_result, valid_mask, min_patch_size
                )
            else:
                processed_result = remove_small_patches_fast(
                    processed_result, valid_mask, min_patch_size
                )
    
    # æ­¥éª¤2: å¹³æ»‘å¤„ç†
    if smoothing_iterations > 0:
        with PerformanceTimer("å¹³æ»‘å¤„ç†"):
            for i in range(smoothing_iterations):
                if smoothing_iterations > 1:
                    print(f"  è¿­ä»£ {i+1}/{smoothing_iterations}...")
                processed_result = majority_filter_fast(processed_result, valid_mask)
    
    return processed_result

# ==================== å…¶ä»–å‡½æ•°ä¿æŒä¸å˜ ====================

def list_classification_methods():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„åˆ†ç±»æ–¹æ³•"""
    methods = {
        'minibatch_kmeans': {
            'name': 'MiniBatch K-Means',
            'desc': 'æœ€å¿«é€Ÿçš„æ–¹æ³•ï¼Œé€‚åˆå¤§æ•°æ®é›†',
            'speed': 'âš¡âš¡âš¡âš¡âš¡',
            'accuracy': 'â˜…â˜…â˜…â˜†â˜†',
            'need_n_clusters': True,
            'best_for': 'å¤§æ•°æ®é›†å¿«é€Ÿåˆ†ç±»',
            'available': True
        },
        'kmeans': {
            'name': 'K-Meansèšç±»',
            'desc': 'ç»å…¸èšç±»ç®—æ³•ï¼Œé€Ÿåº¦å’Œç²¾åº¦å¹³è¡¡',
            'speed': 'âš¡âš¡âš¡âš¡â˜†',
            'accuracy': 'â˜…â˜…â˜…â˜…â˜†',
            'need_n_clusters': True,
            'best_for': 'æ ‡å‡†é¥æ„Ÿåˆ†ç±»',
            'available': True
        },
        'birch': {
            'name': 'Birchèšç±»',
            'desc': 'å±‚æ¬¡èšç±»ï¼Œå†…å­˜æ•ˆç‡é«˜',
            'speed': 'âš¡âš¡âš¡âš¡â˜†',
            'accuracy': 'â˜…â˜…â˜…â˜…â˜†',
            'need_n_clusters': True,
            'best_for': 'å¤§æ•°æ®é›†å±‚æ¬¡åˆ†ç±»',
            'available': True
        },
        'gmm': {
            'name': 'é«˜æ–¯æ··åˆæ¨¡å‹',
            'desc': 'æ¦‚ç‡æ¨¡å‹ï¼Œç²¾åº¦é«˜',
            'speed': 'âš¡âš¡âš¡â˜†â˜†',
            'accuracy': 'â˜…â˜…â˜…â˜…â˜…',
            'need_n_clusters': True,
            'best_for': 'ç²¾ç»†åˆ†ç±»',
            'available': True
        },
        'fuzzy_cmeans': {
            'name': 'æ¨¡ç³ŠCå‡å€¼',
            'desc': 'é¥æ„Ÿå¸¸ç”¨ï¼Œè½¯åˆ†ç±»',
            'speed': 'âš¡âš¡âš¡â˜†â˜†',
            'accuracy': 'â˜…â˜…â˜…â˜…â˜…',
            'need_n_clusters': True,
            'best_for': 'è¾¹ç•Œæ¨¡ç³Šçš„åœ°ç‰©åˆ†ç±»',
            'available': FUZZY_AVAILABLE
        },
        'spectral': {
            'name': 'è°±èšç±»',
            'desc': 'åŸºäºå›¾è®ºï¼Œå¤„ç†å¤æ‚ç»“æ„',
            'speed': 'âš¡âš¡â˜†â˜†â˜†',
            'accuracy': 'â˜…â˜…â˜…â˜…â˜…',
            'need_n_clusters': True,
            'best_for': 'å¤æ‚ç©ºé—´ç»“æ„',
            'available': True
        },
        'dbscan': {
            'name': 'DBSCAN',
            'desc': 'åŸºäºå¯†åº¦ï¼Œè‡ªåŠ¨ç¡®å®šç±»åˆ«æ•°',
            'speed': 'âš¡âš¡âš¡â˜†â˜†',
            'accuracy': 'â˜…â˜…â˜…â˜†â˜†',
            'need_n_clusters': False,
            'best_for': 'å™ªå£°æ•°æ®å¤„ç†',
            'available': True
        },
        'optics': {
            'name': 'OPTICS',
            'desc': 'DBSCANæ”¹è¿›ç‰ˆï¼Œå¤„ç†å˜å¯†åº¦',
            'speed': 'âš¡âš¡â˜†â˜†â˜†',
            'accuracy': 'â˜…â˜…â˜…â˜…â˜†',
            'need_n_clusters': False,
            'best_for': 'å˜å¯†åº¦æ•°æ®',
            'available': True
        },
        'meanshift': {
            'name': 'Mean Shift',
            'desc': 'å‡å€¼æ¼‚ç§»ï¼Œè‡ªåŠ¨ç¡®å®šç±»åˆ«',
            'speed': 'âš¡âš¡â˜†â˜†â˜†',
            'accuracy': 'â˜…â˜…â˜…â˜…â˜†',
            'need_n_clusters': False,
            'best_for': 'è‡ªç„¶èšç±»å‘ç°',
            'available': True
        },
        'isodata': {
            'name': 'ISODATA',
            'desc': 'é¥æ„Ÿç»å…¸ç®—æ³•ï¼Œå¯å˜ç±»åˆ«æ•°',
            'speed': 'âš¡âš¡âš¡â˜†â˜†',
            'accuracy': 'â˜…â˜…â˜…â˜…â˜†',
            'need_n_clusters': True,
            'best_for': 'é¥æ„Ÿå½±åƒæ ‡å‡†åˆ†ç±»',
            'available': True
        },
        'hierarchical': {
            'name': 'å±‚æ¬¡èšç±»',
            'desc': 'å‡èšå±‚æ¬¡èšç±»',
            'speed': 'âš¡âš¡â˜†â˜†â˜†',
            'accuracy': 'â˜…â˜…â˜…â˜…â˜†',
            'need_n_clusters': True,
            'best_for': 'ä¸­å°æ•°æ®é›†',
            'available': True
        },
    }
    return methods

def print_methods_table():
    """æ‰“å°æ–¹æ³•å¯¹æ¯”è¡¨"""
    methods = list_classification_methods()
    
    print("\n" + "="*105)
    print(f"{'åºå·':<4} {'æ–¹æ³•å':<22} {'é€Ÿåº¦':<15} {'ç²¾åº¦':<15} {'é€‚ç”¨åœºæ™¯':<30} {'çŠ¶æ€':<8}")
    print("="*105)
    
    idx = 1
    for key, info in methods.items():
        if info['available']:
            status = "âœ“å¯ç”¨"
            print(f"{idx:<4} {info['name']:<22} {info['speed']:<15} {info['accuracy']:<15} {info['best_for']:<30} {status:<8}")
            idx += 1
        else:
            status = "âœ—ä¸å¯ç”¨"
            print(f"{'--':<4} {info['name']:<22} {info['speed']:<15} {info['accuracy']:<15} {info['best_for']:<30} {status:<8}")
    
    print("="*105)
    print("è¯´æ˜: âš¡=é€Ÿåº¦ â˜…=ç²¾åº¦ | æ¨è: å¤§æ•°æ®ç”¨MiniBatch, é«˜ç²¾åº¦ç”¨GMM, é¥æ„Ÿç”¨ISODATA")
    if NUMBA_AVAILABLE:
        print("âœ“ NumbaåŠ é€Ÿå·²å¯ç”¨ - åå¤„ç†å°†æ˜¾è‘—åŠ å¿«")
    else:
        print("ğŸ’¡ æç¤º: å®‰è£…numbaå¯å¤§å¹…åŠ é€Ÿåå¤„ç† (pip install numba)")
    print("="*105)

def create_valid_data_mask(bands_data, nodata_value=None):
    """åˆ›å»ºæœ‰æ•ˆæ•°æ®æ©è†œ"""
    print("åˆ›å»ºæœ‰æ•ˆæ•°æ®æ©è†œ...")
    
    if nodata_value is not None:
        invalid_mask = np.any(bands_data == nodata_value, axis=0)
    else:
        invalid_mask = (
            np.all(bands_data == 0, axis=0) |
            np.any(np.isnan(bands_data), axis=0) |
            np.any(np.isinf(bands_data), axis=0)
        )
    
    valid_mask = ~invalid_mask
    valid_percentage = np.sum(valid_mask) / valid_mask.size * 100
    print(f"âœ“ æœ‰æ•ˆåƒç´ : {valid_percentage:.2f}%")
    
    return valid_mask

def sample_data_for_training(data_clean, max_samples=50000, random_state=42):
    """æ™ºèƒ½é‡‡æ ·"""
    n_samples = len(data_clean)
    
    if n_samples <= max_samples:
        print(f"ä½¿ç”¨å…¨éƒ¨æ•°æ® ({n_samples:,}ç‚¹)")
        return data_clean, None
    
    print(f"é‡‡æ · {max_samples:,}/{n_samples:,} ç‚¹è®­ç»ƒ")
    np.random.seed(random_state)
    indices = np.random.choice(n_samples, max_samples, replace=False)
    
    return data_clean[indices], indices

def calculate_pixel_area(transform, crs=None):
    """è®¡ç®—å•ä¸ªåƒç´ çš„é¢ç§¯ï¼ˆå¹³æ–¹ç±³ï¼‰"""
    print("\n" + "="*60)
    print("åƒç´ é¢ç§¯è®¡ç®—")
    print("="*60)
    
    pixel_width = abs(transform[0])
    pixel_height = abs(transform[4])
    
    print(f"åƒç´ å®½åº¦: {pixel_width}")
    print(f"åƒç´ é«˜åº¦: {pixel_height}")
    
    if crs is not None:
        print(f"åæ ‡ç³»ç»Ÿ: {crs}")
        
        if crs.is_geographic:
            print("âš ï¸  æ£€æµ‹åˆ°åœ°ç†åæ ‡ç³»ï¼ˆåº¦ï¼‰ï¼Œéœ€è¦è½¬æ¢ä¸ºç±³")
            pixel_width_m = pixel_width * 111320
            pixel_height_m = pixel_height * 110540
            pixel_area = pixel_width_m * pixel_height_m
        else:
            print("âœ“ æ£€æµ‹åˆ°æŠ•å½±åæ ‡ç³»ï¼ˆç±³ï¼‰")
            pixel_area = pixel_width * pixel_height
    else:
        print("âš ï¸  æœªæä¾›åæ ‡ç³»ä¿¡æ¯ï¼Œå‡è®¾å•ä½ä¸ºç±³")
        pixel_area = pixel_width * pixel_height
    
    print(f"\nå•ä¸ªåƒç´ é¢ç§¯: {pixel_area:.2f} å¹³æ–¹ç±³")
    print(f"           = {pixel_area/10000:.6f} å…¬é¡·")
    print(f"           = {pixel_area/1000000:.8f} å¹³æ–¹åƒç±³")
    print("="*60)
    
    return pixel_area

def calculate_class_areas_enhanced(classification_result, valid_mask, pixel_area, crs=None):
    """å¢å¼ºçš„é¢ç§¯è®¡ç®—å‡½æ•°"""
    print("\n" + "="*80)
    print("é¢ç§¯ç»Ÿè®¡ (å¤šå•ä½)")
    print("="*80)
    
    if pixel_area <= 0 or not np.isfinite(pixel_area):
        print(f"âš ï¸  é”™è¯¯: åƒç´ é¢ç§¯æ— æ•ˆ (pixel_area = {pixel_area})")
        pixel_area = 900.0
    
    print(f"ä½¿ç”¨çš„åƒç´ é¢ç§¯: {pixel_area:.2f} å¹³æ–¹ç±³")
    
    valid_data = classification_result[valid_mask]
    
    if len(valid_data) == 0:
        print("âš ï¸  è­¦å‘Š: æ²¡æœ‰æœ‰æ•ˆæ•°æ®ç”¨äºç»Ÿè®¡")
        return []
    
    unique_classes, counts = np.unique(valid_data, return_counts=True)
    valid_indices = (unique_classes > 0) & np.isfinite(unique_classes) & (counts > 0)
    unique_classes = unique_classes[valid_indices]
    counts = counts[valid_indices]
    
    if len(unique_classes) == 0:
        print("âš ï¸  è­¦å‘Š: æ²¡æœ‰æœ‰æ•ˆçš„åˆ†ç±»ç±»åˆ«")
        return []
    
    total_pixels = np.sum(counts)
    total_area_m2 = float(total_pixels) * pixel_area
    total_area_km2 = total_area_m2 / 1_000_000
    total_area_ha = total_area_m2 / 10_000
    
    print(f"\næ€»æœ‰æ•ˆåŒºåŸŸ:")
    print(f"  åƒç´ æ•°é‡: {total_pixels:,}")
    print(f"  é¢ç§¯: {total_area_m2:,.2f} å¹³æ–¹ç±³")
    print(f"      = {total_area_km2:,.4f} å¹³æ–¹åƒç±³")
    print(f"      = {total_area_ha:,.2f} å…¬é¡·")
    
    print("\n" + "-"*80)
    print(f"{'ç±»åˆ«':<8} {'åƒç´ æ•°':<12} {'å¹³æ–¹ç±³':<15} {'å¹³æ–¹åƒç±³':<12} {'å…¬é¡·':<12} {'å æ¯”%':<10}")
    print("-"*80)
    
    area_stats = []
    
    for cluster_id, cluster_pixels in zip(unique_classes, counts):
        if not np.isfinite(cluster_pixels) or cluster_pixels <= 0:
            continue
        
        area_m2 = float(cluster_pixels) * pixel_area
        area_km2 = area_m2 / 1_000_000
        area_ha = area_m2 / 10_000
        percentage = (float(cluster_pixels) / total_pixels) * 100
        
        if not all(np.isfinite([area_m2, area_km2, area_ha, percentage])):
            continue
        
        print(f"{int(cluster_id):<8} {int(cluster_pixels):<12,} {area_m2:<15,.2f} {area_km2:<12,.4f} {area_ha:<12,.2f} {percentage:<10,.2f}")
        
        area_stats.append({
            'ç±»åˆ«': int(cluster_id),
            'åƒç´ æ•°é‡': int(cluster_pixels),
            'é¢ç§¯_å¹³æ–¹ç±³': round(float(area_m2), 2),
            'é¢ç§¯_å¹³æ–¹åƒç±³': round(float(area_km2), 6),
            'é¢ç§¯_å…¬é¡·': round(float(area_ha), 2),
            'å æ¯”_ç™¾åˆ†æ¯”': round(float(percentage), 2)
        })
    
    print("="*80)
    
    return area_stats

# [åˆ†ç±»ç®—æ³•å‡½æ•°ä¿æŒä¸å˜ - è¿™é‡Œçœç•¥ä»¥èŠ‚çœç©ºé—´]

def apply_kmeans(data, n_clusters, use_sampling):
    """K-Meansèšç±»"""
    sample_data, sample_indices = sample_data_for_training(data) if use_sampling else (data, None)
    
    model = KMeans(
        n_clusters=n_clusters,
        random_state=42,
        n_init=5,
        max_iter=200,
        algorithm='elkan'
    )
    
    if sample_indices is not None:
        model.fit(sample_data)
        labels = model.predict(data)
    else:
        labels = model.fit_predict(data)
    
    return labels, model

def apply_minibatch_kmeans(data, n_clusters, use_sampling):
    """MiniBatch K-Means"""
    batch_size = min(2048, len(data) // 20)
    
    model = MiniBatchKMeans(
        n_clusters=n_clusters,
        random_state=42,
        batch_size=batch_size,
        max_iter=100,
        n_init=3
    )
    
    labels = model.fit_predict(data)
    return labels, model

def apply_gmm(data, n_clusters, use_sampling):
    """é«˜æ–¯æ··åˆæ¨¡å‹"""
    sample_data, sample_indices = sample_data_for_training(data, 30000) if use_sampling else (data, None)
    
    model = GaussianMixture(
        n_components=n_clusters,
        random_state=42,
        max_iter=50,
        covariance_type='diag'
    )
    
    if sample_indices is not None:
        model.fit(sample_data)
        labels = model.predict(data)
    else:
        labels = model.fit_predict(data)
    
    return labels, model

def apply_fuzzy_cmeans(data, n_clusters, use_sampling):
    """æ¨¡ç³ŠCå‡å€¼èšç±»"""
    print("  ä½¿ç”¨æ¨¡ç³ŠCå‡å€¼ç®—æ³• (é¥æ„Ÿç»å…¸æ–¹æ³•)")
    
    sample_data, sample_indices = sample_data_for_training(data, 20000) if use_sampling else (data, None)
    
    if FUZZY_AVAILABLE:
        data_T = sample_data.T
        
        cntr, u, u0, d, jm, p, fpc = fuzz.cluster.cmeans(
            data_T,
            c=n_clusters,
            m=2,
            error=0.005,
            maxiter=100,
            init=None
        )
        
        if sample_indices is not None:
            u_full, u0, d, jm, p, fpc = fuzz.cluster.cmeans_predict(
                data.T, cntr, 2, error=0.005, maxiter=100
            )
            labels = np.argmax(u_full, axis=0)
        else:
            labels = np.argmax(u, axis=0)
        
        print(f"  æ¨¡ç³Šåˆ†å‰²ç³»æ•° (FPC): {fpc:.3f}")
        
        class FuzzyCMeansModel:
            def __init__(self, centers):
                self.cluster_centers_ = centers
        
        return labels, FuzzyCMeansModel(cntr)
    else:
        print("  è­¦å‘Š: scikit-fuzzyæœªå®‰è£…ï¼Œä½¿ç”¨KMeansæ›¿ä»£")
        return apply_kmeans(data, n_clusters, use_sampling)

def apply_spectral(data, n_clusters, use_sampling):
    """è°±èšç±»"""
    print("  è­¦å‘Š: è°±èšç±»è®¡ç®—é‡å¤§ï¼Œå¼ºåˆ¶é‡‡æ ·")
    
    max_samples = min(10000, len(data))
    sample_data, sample_indices = sample_data_for_training(data, max_samples)
    
    model = SpectralClustering(
        n_clusters=n_clusters,
        random_state=42,
        affinity='nearest_neighbors',
        n_neighbors=10,
        assign_labels='kmeans'
    )
    
    sample_labels = model.fit_predict(sample_data)
    
    print("  ä½¿ç”¨KMeansæ‰©å±•åˆ†ç±»ç»“æœ...")
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=3)
    
    centers = []
    for i in range(n_clusters):
        mask = sample_labels == i
        if np.sum(mask) > 0:
            centers.append(np.mean(sample_data[mask], axis=0))
        else:
            centers.append(sample_data[np.random.randint(len(sample_data))])
    
    kmeans.cluster_centers_ = np.array(centers)
    labels = kmeans.predict(data)
    
    return labels, kmeans

def apply_dbscan(data, use_sampling):
    """DBSCAN"""
    print("  DBSCANè‡ªåŠ¨ç¡®å®šç±»åˆ«æ•°")
    
    sample_data, sample_indices = sample_data_for_training(data, 10000) if use_sampling else (data, None)
    
    from sklearn.neighbors import NearestNeighbors
    neighbors = NearestNeighbors(n_neighbors=10)
    neighbors.fit(sample_data if sample_indices is not None else data)
    distances, indices = neighbors.kneighbors(sample_data if sample_indices is not None else data)
    distances = np.sort(distances[:, -1])
    eps = np.percentile(distances, 90) * 0.5
    
    print(f"  è‡ªåŠ¨ä¼°è®¡ eps={eps:.3f}")
    
    model = DBSCAN(eps=eps, min_samples=5, n_jobs=-1)
    
    if sample_indices is not None:
        sample_labels = model.fit_predict(sample_data)
        
        from sklearn.neighbors import KNeighborsClassifier
        valid_mask = sample_labels >= 0
        if np.sum(valid_mask) > 0:
            knn = KNeighborsClassifier(n_neighbors=3)
            knn.fit(sample_data[valid_mask], sample_labels[valid_mask])
            labels = knn.predict(data)
        else:
            labels = np.zeros(len(data), dtype=int)
    else:
        labels = model.fit_predict(data)
    
    labels[labels == -1] = labels.max() + 1
    n_clusters = len(np.unique(labels))
    print(f"  æ£€æµ‹åˆ° {n_clusters} ä¸ªç±»åˆ«")
    
    return labels, model

def apply_optics(data, use_sampling):
    """OPTICS"""
    print("  OPTICSè‡ªåŠ¨ç¡®å®šç±»åˆ«æ•°")
    
    sample_data, sample_indices = sample_data_for_training(data, 8000) if use_sampling else (data, None)
    
    model = OPTICS(min_samples=10, max_eps=2.0, n_jobs=-1)
    
    if sample_indices is not None:
        sample_labels = model.fit_predict(sample_data)
        
        from sklearn.neighbors import KNeighborsClassifier
        valid_mask = sample_labels >= 0
        if np.sum(valid_mask) > 0:
            knn = KNeighborsClassifier(n_neighbors=3)
            knn.fit(sample_data[valid_mask], sample_labels[valid_mask])
            labels = knn.predict(data)
        else:
            labels = np.zeros(len(data), dtype=int)
    else:
        labels = model.fit_predict(data)
    
    labels[labels == -1] = labels.max() + 1
    n_clusters = len(np.unique(labels))
    print(f"  æ£€æµ‹åˆ° {n_clusters} ä¸ªç±»åˆ«")
    
    return labels, model

def apply_meanshift(data, use_sampling):
    """Mean Shift"""
    print("  Mean Shiftè‡ªåŠ¨ç¡®å®šç±»åˆ«æ•°")
    print("  è­¦å‘Š: è®¡ç®—é‡å¤§ï¼Œå¼ºåˆ¶é‡‡æ ·")
    
    sample_data, _ = sample_data_for_training(data, 5000)
    
    from sklearn.cluster import estimate_bandwidth
    bandwidth = estimate_bandwidth(sample_data, quantile=0.2, n_samples=1000)
    
    print(f"  è‡ªåŠ¨ä¼°è®¡ bandwidth={bandwidth:.3f}")
    
    model = MeanShift(bandwidth=bandwidth, bin_seeding=True, n_jobs=-1)
    sample_labels = model.fit_predict(sample_data)
    
    from sklearn.neighbors import KNeighborsClassifier
    knn = KNeighborsClassifier(n_neighbors=3)
    knn.fit(sample_data, sample_labels)
    labels = knn.predict(data)
    
    n_clusters = len(np.unique(labels))
    print(f"  æ£€æµ‹åˆ° {n_clusters} ä¸ªç±»åˆ«")
    
    return labels, model

def apply_birch(data, n_clusters, use_sampling):
    """Birchèšç±»"""
    model = Birch(
        n_clusters=n_clusters,
        threshold=0.5,
        branching_factor=50
    )
    
    labels = model.fit_predict(data)
    return labels, model

def apply_isodata(data, n_clusters, use_sampling):
    """ISODATAç®—æ³•"""
    print("  ä½¿ç”¨ISODATAç®—æ³• (é¥æ„Ÿç»å…¸æ–¹æ³•)")
    
    sample_data, sample_indices = sample_data_for_training(data, 30000) if use_sampling else (data, None)
    
    max_clusters = n_clusters + 2
    min_clusters = max(2, n_clusters - 2)
    max_iterations = 20
    max_std = 1.0
    min_distance = 0.5
    
    current_centers = KMeans(n_clusters=n_clusters, random_state=42, n_init=3).fit(sample_data).cluster_centers_
    
    for iteration in range(max_iterations):
        from scipy.spatial.distance import cdist
        distances = cdist(sample_data, current_centers)
        labels = np.argmin(distances, axis=1)
        
        new_centers = []
        cluster_stds = []
        
        for i in range(len(current_centers)):
            mask = labels == i
            if np.sum(mask) == 0:
                continue
            
            cluster_data = sample_data[mask]
            center = np.mean(cluster_data, axis=0)
            std = np.std(cluster_data)
            
            new_centers.append(center)
            cluster_stds.append(std)
        
        new_centers = np.array(new_centers)
        
        if len(new_centers) < max_clusters:
            for i, std in enumerate(cluster_stds):
                if std > max_std and len(new_centers) < max_clusters:
                    offset = np.random.randn(new_centers.shape[1]) * std * 0.5
                    new_centers = np.vstack([new_centers, new_centers[i] + offset])
        
        if len(new_centers) > min_clusters:
            distances_centers = cdist(new_centers, new_centers)
            np.fill_diagonal(distances_centers, np.inf)
            min_dist_idx = np.unravel_index(np.argmin(distances_centers), distances_centers.shape)
            
            if distances_centers[min_dist_idx] < min_distance:
                merged_center = (new_centers[min_dist_idx[0]] + new_centers[min_dist_idx[1]]) / 2
                new_centers = np.delete(new_centers, min_dist_idx, axis=0)
                new_centers = np.vstack([new_centers, merged_center])
        
        if len(current_centers) == len(new_centers):
            if np.allclose(current_centers, new_centers, rtol=0.01):
                print(f"  ISODATAæ”¶æ•›äºç¬¬ {iteration+1} æ¬¡è¿­ä»£")
                break
        
        current_centers = new_centers
    
    if sample_indices is not None:
        from scipy.spatial.distance import cdist
        distances_full = cdist(data, current_centers)
        labels = np.argmin(distances_full, axis=1)
    else:
        from scipy.spatial.distance import cdist
        distances = cdist(sample_data, current_centers)
        labels = np.argmin(distances, axis=1)
    
    actual_clusters = len(current_centers)
    print(f"  ISODATAæœ€ç»ˆç±»åˆ«æ•°: {actual_clusters}")
    
    class ISODATAModel:
        def __init__(self, centers):
            self.cluster_centers_ = centers
    
    return labels, ISODATAModel(current_centers)

def apply_hierarchical(data, n_clusters, use_sampling):
    """å±‚æ¬¡èšç±»"""
    sample_data, sample_indices = sample_data_for_training(data, 5000) if use_sampling else (data, None)
    
    model = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')
    
    if sample_indices is not None:
        sample_labels = model.fit_predict(sample_data)
        
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=3)
        centers = []
        for i in range(n_clusters):
            mask = sample_labels == i
            if np.sum(mask) > 0:
                centers.append(np.mean(sample_data[mask], axis=0))
        
        kmeans.cluster_centers_ = np.array(centers)
        labels = kmeans.predict(data)
        return labels, kmeans
    else:
        labels = model.fit_predict(data)
        return labels, model

def apply_classification_method(data_clean, n_clusters, method='minibatch_kmeans', 
                               random_state=42, use_sampling=True):
    """ç»Ÿä¸€çš„åˆ†ç±»æ–¹æ³•æ¥å£"""
    n_samples = len(data_clean)
    print(f"æ•°æ®: {n_samples:,}ç‚¹, ç‰¹å¾: {data_clean.shape[1]}ç»´, ç›®æ ‡: {n_clusters}ç±»")
    
    np.random.seed(random_state)
    
    try:
        if method == 'kmeans':
            labels, model = apply_kmeans(data_clean, n_clusters, use_sampling)
        elif method == 'minibatch_kmeans':
            labels, model = apply_minibatch_kmeans(data_clean, n_clusters, use_sampling)
        elif method == 'gmm':
            labels, model = apply_gmm(data_clean, n_clusters, use_sampling)
        elif method == 'fuzzy_cmeans':
            labels, model = apply_fuzzy_cmeans(data_clean, n_clusters, use_sampling)
        elif method == 'spectral':
            labels, model = apply_spectral(data_clean, n_clusters, use_sampling)
        elif method == 'dbscan':
            labels, model = apply_dbscan(data_clean, use_sampling)
        elif method == 'optics':
            labels, model = apply_optics(data_clean, use_sampling)
        elif method == 'meanshift':
            labels, model = apply_meanshift(data_clean, use_sampling)
        elif method == 'birch':
            labels, model = apply_birch(data_clean, n_clusters, use_sampling)
        elif method == 'isodata':
            labels, model = apply_isodata(data_clean, n_clusters, use_sampling)
        elif method == 'hierarchical':
            labels, model = apply_hierarchical(data_clean, n_clusters, use_sampling)
        else:
            raise ValueError(f"æœªçŸ¥æ–¹æ³•: {method}")
        
        if labels.min() == 0:
            labels = labels + 1
        
        actual_n_clusters = len(np.unique(labels))
        print(f"âœ“ åˆ†ç±»å®Œæˆ! å®é™…ç±»åˆ«: {actual_n_clusters}")
        
        return labels, model, actual_n_clusters
        
    except Exception as e:
        print(f"âœ— åˆ†ç±»å¤±è´¥: {str(e)}")
        raise

def landsat8_unsupervised_classification(input_file, output_file, n_clusters=5,
                                        nodata_value=None, post_process=True,
                                        min_patch_size=50, smoothing_iterations=1,
                                        method='minibatch_kmeans', use_sampling=True):
    """ä¸»åˆ†ç±»å‡½æ•° - ä½¿ç”¨ä¼˜åŒ–çš„åå¤„ç†"""
    print("="*80)
    print(" "*15 + "Landsat 8 éç›‘ç£åˆ†ç±»ç³»ç»Ÿ (é«˜æ€§èƒ½ç‰ˆ)")
    print("="*80)
    
    total_start = time.time()
    
    with PerformanceTimer("è¯»å–å½±åƒæ•°æ®"):
        with rasterio.open(input_file) as src:
            bands_data = src.read()
            profile = src.profile
            transform = src.transform
            crs = src.crs
            
            if nodata_value is None and src.nodata is not None:
                nodata_value = src.nodata
            
            print(f"  å½¢çŠ¶: {bands_data.shape}, æ³¢æ®µ: {src.count}, å°ºå¯¸: {src.width}Ã—{src.height}")
    
    pixel_area = calculate_pixel_area(transform, crs)
    
    with PerformanceTimer("åˆ›å»ºæœ‰æ•ˆæ•°æ®æ©è†œ"):
        valid_mask_2d = create_valid_data_mask(bands_data, nodata_value)
    
    with PerformanceTimer("æ•°æ®é¢„å¤„ç†"):
        height, width = bands_data.shape[1], bands_data.shape[2]
        n_bands = bands_data.shape[0]
        
        data_2d = bands_data.reshape(n_bands, -1).T
        valid_mask_1d = valid_mask_2d.ravel()
        data_valid = data_2d[valid_mask_1d]
        
        finite_mask = np.all(np.isfinite(data_valid), axis=1)
        data_clean_temp = data_valid[finite_mask]
        
        print(f"  æœ‰æ•ˆæ•°æ®: {len(data_clean_temp):,}ç‚¹")
        
        scaler = StandardScaler()
        data_clean = scaler.fit_transform(data_clean_temp)
    
    with PerformanceTimer(f"æ‰§è¡Œåˆ†ç±» - {method}"):
        labels, model, actual_n_clusters = apply_classification_method(
            data_clean, n_clusters, method, use_sampling=use_sampling
        )
    
    with PerformanceTimer("é‡å»ºåˆ†ç±»ç»“æœ"):
        full_labels_temp = np.zeros(len(data_valid), dtype=np.uint8)
        full_labels_temp[finite_mask] = labels
        
        full_labels = np.zeros(height * width, dtype=np.uint8)
        full_labels[valid_mask_1d] = full_labels_temp
        
        classification_result = full_labels.reshape(height, width)
    
    # ä½¿ç”¨ä¼˜åŒ–çš„åå¤„ç†
    if post_process:
        classification_result = post_process_classification_fast(
            classification_result, valid_mask_2d, min_patch_size, smoothing_iterations
        )
    
    with PerformanceTimer("ä¿å­˜ç»“æœ"):
        profile.update({
            'dtype': rasterio.uint8,
            'count': 1,
            'compress': 'lzw',
            'nodata': 0
        })
        
        with rasterio.open(output_file, 'w', **profile) as dst:
            dst.write(classification_result, 1)
    
    area_stats = calculate_class_areas_enhanced(classification_result, valid_mask_2d, pixel_area, crs)
    
    total_time = time.time() - total_start
    print("\n" + "="*80)
    print(f"âœ“ å®Œæˆ! æ€»è€—æ—¶: {total_time:.2f}ç§’")
    print("="*80)
    
    return classification_result, model, valid_mask_2d, area_stats, pixel_area

# [å¯è§†åŒ–å‡½æ•°ä¿æŒä¸å˜]

def plot_classification_result(classification_result, valid_mask, title_suffix="",
                               save_path='classification_result.png', dpi=150):
    """å¯è§†åŒ–åˆ†ç±»ç»“æœ"""
    with PerformanceTimer("ç”Ÿæˆåˆ†ç±»å›¾"):
        plot_data = classification_result.astype(float)
        plot_data[~valid_mask] = np.nan
        
        unique_classes = np.unique(classification_result[valid_mask])
        unique_classes = unique_classes[unique_classes > 0]
        n_classes = len(unique_classes)
        
        if n_classes == 0:
            print("âš ï¸  è­¦å‘Š: æ²¡æœ‰æœ‰æ•ˆçš„åˆ†ç±»ç»“æœå¯è§†åŒ–")
            return
        
        colors = ['#228B22', '#32CD32', '#90EE90', '#FFD700', '#FFA500',
                  '#8B4513', '#4169E1', '#87CEEB', '#808080', '#DC143C',
                  '#9370DB', '#FF69B4', '#00CED1', '#FF6347', '#4682B4']
        
        if n_classes > len(colors):
            import matplotlib.cm as cm
            cmap_colors = cm.get_cmap('tab20', n_classes)
            colors = [cmap_colors(i) for i in range(n_classes)]
        
        cmap = ListedColormap(colors[:n_classes])
        
        plt.figure(figsize=(14, 10))
        im = plt.imshow(plot_data, cmap=cmap, interpolation='nearest')
        
        cbar = plt.colorbar(im, label='åœŸåœ°ç±»åˆ«', shrink=0.7)
        cbar.set_ticks(unique_classes)
        cbar.set_ticklabels([f'ç±»åˆ« {int(i)}' for i in unique_classes])
        
        title = f'Landsat 8 åˆ†ç±»ç»“æœ ({n_classes}ç±»)'
        if title_suffix:
            title += f" - {title_suffix}"
        plt.title(title, fontsize=14, fontweight='bold')
        
        plt.xlabel('åˆ—', fontsize=11)
        plt.ylabel('è¡Œ', fontsize=11)
        
        legend_elements = [
            Patch(facecolor=colors[i], label=f'ç±»åˆ« {int(unique_classes[i])}')
            for i in range(n_classes)
        ]
        plt.legend(handles=legend_elements, loc='upper right', 
                  bbox_to_anchor=(1.15, 1), title="ç±»å‹", fontsize=9)
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=dpi, bbox_inches='tight')
        print(f"  ä¿å­˜: {save_path}")
        plt.close()

def plot_area_distribution(area_stats, method_name, save_path='é¢ç§¯åˆ†å¸ƒ.png', dpi=150):
    """é¢ç§¯åˆ†å¸ƒå›¾"""
    if not area_stats or len(area_stats) == 0:
        print("âš ï¸  è­¦å‘Š: æ²¡æœ‰é¢ç§¯ç»Ÿè®¡æ•°æ®å¯ç»˜åˆ¶")
        return
    
    with PerformanceTimer("ç”Ÿæˆé¢ç§¯å›¾"):
        classes = []
        areas_km2 = []
        areas_ha = []
        percentages = []
        
        for stat in area_stats:
            area_km2 = stat['é¢ç§¯_å¹³æ–¹åƒç±³']
            area_ha = stat['é¢ç§¯_å…¬é¡·']
            pct = stat['å æ¯”_ç™¾åˆ†æ¯”']
            
            if np.isfinite(area_km2) and np.isfinite(pct) and area_km2 > 0:
                classes.append(f'ç±»åˆ«{stat["ç±»åˆ«"]}')
                areas_km2.append(float(area_km2))
                areas_ha.append(float(area_ha))
                percentages.append(float(pct))
        
        if len(areas_km2) == 0:
            print("âš ï¸  è­¦å‘Š: æ²¡æœ‰æœ‰æ•ˆçš„é¢ç§¯æ•°æ®å¯ç»˜åˆ¶")
            return
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(18, 14))
        
        colors_array = plt.cm.Set3(np.linspace(0, 1, len(classes)))
        
        # æŸ±çŠ¶å›¾ - å¹³æ–¹åƒç±³
        bars1 = ax1.bar(classes, areas_km2, color=colors_array, edgecolor='black')
        ax1.set_title(f'{method_name} - é¢ç§¯åˆ†å¸ƒ (kmÂ²)', fontsize=12, fontweight='bold')
        ax1.set_xlabel('ç±»åˆ«', fontsize=10)
        ax1.set_ylabel('é¢ç§¯ (kmÂ²)', fontsize=10)
        ax1.tick_params(axis='x', rotation=45)
        ax1.grid(axis='y', alpha=0.3)
        
        for bar, area, pct in zip(bars1, areas_km2, percentages):
            height = bar.get_height()
            if np.isfinite(height) and height > 0:
                ax1.text(bar.get_x() + bar.get_width()/2., height,
                        f'{area:.2f}\n({pct:.1f}%)',
                        ha='center', va='bottom', fontsize=8)
        
        # æŸ±çŠ¶å›¾ - å…¬é¡·
        bars2 = ax2.bar(classes, areas_ha, color=colors_array, edgecolor='black')
        ax2.set_title(f'{method_name} - é¢ç§¯åˆ†å¸ƒ (å…¬é¡·)', fontsize=12, fontweight='bold')
        ax2.set_xlabel('ç±»åˆ«', fontsize=10)
        ax2.set_ylabel('é¢ç§¯ (å…¬é¡·)', fontsize=10)
        ax2.tick_params(axis='x', rotation=45)
        ax2.grid(axis='y', alpha=0.3)
        
        for bar, area, pct in zip(bars2, areas_ha, percentages):
            height = bar.get_height()
            if np.isfinite(height) and height > 0:
                ax2.text(bar.get_x() + bar.get_width()/2., height,
                        f'{area:.1f}\n({pct:.1f}%)',
                        ha='center', va='bottom', fontsize=8)
        
        # é¥¼å›¾
        try:
            wedges, texts, autotexts = ax3.pie(
                areas_km2, labels=classes, autopct='%1.1f%%',
                colors=colors_array, startangle=90
            )
            ax3.set_title(f'{method_name} - é¢ç§¯å æ¯”', fontsize=12, fontweight='bold')
            
            for autotext in autotexts:
                autotext.set_color('white')
                autotext.set_fontweight('bold')
        except Exception as e:
            ax3.text(0.5, 0.5, 'ç»˜å›¾å¤±è´¥', ha='center', va='center', fontsize=14)
        
        # è¡¨æ ¼
        table_data = []
        for cls, km2, ha, pct in zip(classes, areas_km2, areas_ha, percentages):
            table_data.append([cls, f'{km2:.4f}', f'{ha:.2f}', f'{pct:.2f}%'])
        
        ax4.axis('tight')
        ax4.axis('off')
        table = ax4.table(
            cellText=table_data,
            colLabels=['ç±»åˆ«', 'é¢ç§¯(kmÂ²)', 'é¢ç§¯(å…¬é¡·)', 'å æ¯”(%)'],
            cellLoc='center',
            loc='center',
            colColours=['lightgray']*4
        )
        table.auto_set_font_size(False)
        table.set_fontsize(9)
        table.scale(1, 2)
        ax4.set_title('è¯¦ç»†æ•°æ®', fontsize=12, fontweight='bold', pad=20)
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=dpi, bbox_inches='tight')
        print(f"  ä¿å­˜: {save_path}")
        plt.close()

def save_area_statistics(area_stats, output_file="é¢ç§¯ç»Ÿè®¡.csv"):
    """ä¿å­˜ç»Ÿè®¡"""
    if not area_stats or len(area_stats) == 0:
        print("âš ï¸  è­¦å‘Š: æ²¡æœ‰ç»Ÿè®¡æ•°æ®å¯ä¿å­˜")
        return None
    
    try:
        df = pd.DataFrame(area_stats)
        df = df.dropna()
        
        if df.empty:
            print("âš ï¸  è­¦å‘Š: æ¸…ç†åç»Ÿè®¡æ•°æ®ä¸ºç©º")
            return None
        
        df = df.sort_values('é¢ç§¯_å¹³æ–¹åƒç±³', ascending=False)
        df.to_csv(output_file, index=False, encoding='utf-8-sig', float_format='%.6f')
        print(f"  ä¿å­˜: {output_file}")
        
        try:
            excel_file = output_file.replace('.csv', '.xlsx')
            df.to_excel(excel_file, index=False, float_format='%.6f')
            print(f"  ä¿å­˜: {excel_file}")
        except:
            pass
        
        return df
    except Exception as e:
        print(f"âš ï¸  è­¦å‘Š: ä¿å­˜ç»Ÿè®¡æ•°æ®å¤±è´¥: {str(e)}")
        return None

def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*80)
    print(" "*10 + "Landsat 8 éç›‘ç£åˆ†ç±»ç³»ç»Ÿ v3.0 (é«˜æ€§èƒ½ç‰ˆ)")
    print("="*80 + "\n")
    
    input_file = r"D:\code313\Geo_programe\rasterio\data\XZ_SQ_L8_2024.tif"
    
    if not os.path.exists(input_file):
        print(f"âœ— æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        return
    
    print_methods_table()
    
    methods = list_classification_methods()
    available_methods = {k: v for k, v in methods.items() if v['available']}
    
    method_input = input("\nè¯·é€‰æ‹©æ–¹æ³• (è¾“å…¥åºå·æˆ–åç§°ï¼Œé»˜è®¤minibatch_kmeans): ").strip()
    
    if not method_input:
        method = 'minibatch_kmeans'
    elif method_input.isdigit():
        available_keys = list(available_methods.keys())
        idx = int(method_input) - 1
        if 0 <= idx < len(available_keys):
            method = available_keys[idx]
        else:
            method = 'minibatch_kmeans'
    elif method_input.lower() in available_methods:
        method = method_input.lower()
    else:
        method = 'minibatch_kmeans'
    
    if method not in available_methods:
        print(f"âš ï¸  æ–¹æ³• {method} ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤æ–¹æ³•")
        method = 'minibatch_kmeans'
    
    method_info = methods[method]
    print(f"\nâœ“ é€‰æ‹©: {method_info['name']}")
    
    if method_info['need_n_clusters']:
        n_input = input(f"\nåˆ†ç±»æ•°é‡ (2-15ï¼Œé»˜è®¤6): ").strip()
        try:
            n_clusters = int(n_input) if n_input else 6
            n_clusters = max(2, min(15, n_clusters))
        except:
            n_clusters = 6
    else:
        n_clusters = 6
    
    print(f"âœ“ ç›®æ ‡ç±»åˆ«: {n_clusters}")
    
    post_input = input("\nåå¤„ç†? (y/nï¼Œé»˜è®¤y): ").strip().lower()
    post_process = post_input != 'n'
    
    if post_process:
        min_patch_size = 300
        smoothing_iterations = 1
        print(f"âœ“ åå¤„ç†: æœ€å°æ–‘å—={min_patch_size}, å¹³æ»‘={smoothing_iterations}")
    else:
        min_patch_size = 0
        smoothing_iterations = 0
    
    sampling_input = input("\nå¤§æ•°æ®é‡‡æ ·åŠ é€Ÿ? (y/nï¼Œé»˜è®¤y): ").strip().lower()
    use_sampling = sampling_input != 'n'
    print(f"âœ“ é‡‡æ ·: {'æ˜¯' if use_sampling else 'å¦'}")
    
    output_file = f"classification_{method}_{n_clusters}classes.tif"
    
    try:
        classification_result, model, valid_mask, area_stats, pixel_area = \
            landsat8_unsupervised_classification(
                input_file, output_file, n_clusters,
                post_process=post_process,
                min_patch_size=min_patch_size,
                smoothing_iterations=smoothing_iterations,
                method=method,
                use_sampling=use_sampling
            )
        
        csv_file = f"ç»Ÿè®¡_{method}.csv"
        df = save_area_statistics(area_stats, csv_file)
        
        plot_file = f"åˆ†ç±»_{method}.png"
        plot_classification_result(classification_result, valid_mask, 
                                  method_info['name'], plot_file)
        
        area_file = f"é¢ç§¯_{method}.png"
        plot_area_distribution(area_stats, method_info['name'], area_file)
        
        print("\n" + "="*80)
        print("âœ“ å…¨éƒ¨å®Œæˆ!")
        print("="*80)
        print("è¾“å‡ºæ–‡ä»¶:")
        print(f"  1. {output_file}")
        if df is not None:
            print(f"  2. {csv_file}")
        print(f"  3. {plot_file}")
        print(f"  4. {area_file}")
        print("="*80 + "\n")
        
    except Exception as e:
        print("\n" + "="*80)
        print("âœ— å¤„ç†å¤±è´¥!")
        print("="*80)
        print(f"é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()